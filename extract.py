# -*- coding: utf-8 -*-
"""test (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PrdTKc_Mk2Tk9IeaR8NZGT5kE19_x8UY
"""

"""# **IMPORT**"""

import argparse
import glob
import os
import shutil
import logging
import random
import warnings
import time
import io
import sys
import cv2
import warnings
import torch

from tqdm import tqdm
from torch import nn, Tensor
from collections import OrderedDict
from typing import Dict, List, Optional, Tuple, Union
from pathlib import Path
from tqdm import tqdm
from icecream import ic


# Data handling and transformations
import numpy as np
from PIL import Image
import cv2
import xml.etree.ElementTree as ET
import os
# Machine learning and deep learning libraries
import torch
import torchvision
from torchvision import transforms as torchtrans
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.generalized_rcnn import GeneralizedRCNN
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
import argparse
import sys
import torch.nn.functional as F

# For image augmentations
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

# Progress bar
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as patches

from torch.utils.data import Dataset, DataLoader
import torch.nn as nn

from torchvision import models, transforms
from collections import OrderedDict

from torchvision.ops import nms
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.faster_rcnn import (
    FastRCNNConvFCHead,
    _default_anchorgen,
    FasterRCNN_ResNet50_FPN_V2_Weights,
    RoIHeads,
    RegionProposalNetwork, RPNHead,
    GeneralizedRCNNTransform,
    MultiScaleRoIAlign,
    TwoMLPHead,
    _ovewrite_value_param
)
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.backbone_utils import (
    _resnet_fpn_extractor,
    _validate_trainable_layers,
)
from torchvision.models.detection.rpn import (
    RegionProposalNetwork,
    RPNHead
)
from torchvision.models import resnet50
from torchvision.models import ResNet50_Weights

## *Identity Class*
"""

# Identity class
class Identity(nn.Module):
    def __init__(self) -> None:
        super(Identity, self).__init__()

    def forward(self, X):
        return X

"""## *Model Class*"""

# roi_head = model.roi_heads
# roi_head

class ExtractingRoiHead(RoIHeads):
    def __init__(self, box_roi_pool, box_head, box_predictor, fg_iou_thresh, bg_iou_thresh, batch_size_per_image, positive_fraction, bbox_reg_weights, score_thresh, nms_thresh, detections_per_img, mask_roi_pool=None, mask_head=None, mask_predictor=None, keypoint_roi_pool=None, keypoint_head=None, keypoint_predictor=None):
        super().__init__(box_roi_pool, box_head, box_predictor, fg_iou_thresh, bg_iou_thresh, batch_size_per_image, positive_fraction, bbox_reg_weights, score_thresh, nms_thresh, detections_per_img, mask_roi_pool, mask_head, mask_predictor, keypoint_roi_pool, keypoint_head, keypoint_predictor)

    def forward(
        self,
        features,  # type: Dict[str, Tensor]
        proposals,  # type: List[Tensor]
        image_shapes,  # type: List[Tuple[int, int]]
        targets=None,  # type: Optional[List[Dict[str, Tensor]]]
    ):
        # type: (...) -> Tuple[List[Dict[str, Tensor]], Dict[str, Tensor]]
        """
        Args:
            features (List[Tensor])
            proposals (List[Tensor[N, 4]])
            image_shapes (List[Tuple[H, W]])
            targets (List[Dict])
        """
        if targets is not None:
            for t in targets:
                # TODO: https://github.com/pytorch/pytorch/issues/26731
                floating_point_types = (torch.float, torch.double, torch.half)
                if not t["boxes"].dtype in floating_point_types:
                    raise TypeError(f"target boxes must of float type, instead got {t['boxes'].dtype}")
                if not t["labels"].dtype == torch.int64:
                    raise TypeError(f"target labels must of int64 type, instead got {t['labels'].dtype}")
                if self.has_keypoint():
                    if not t["keypoints"].dtype == torch.float32:
                        raise TypeError(f"target keypoints must of float type, instead got {t['keypoints'].dtype}")
        # ic(features)
        box_features = self.box_roi_pool(features, proposals, image_shapes)
        # ic(box_features.shape)
        # print(self.box_head)
        box_features = self.box_head(box_features)

        return box_features

class ExtractingGeneralizedRCNN(GeneralizedRCNN):
    def __init__(self, backbone: nn.Module, rpn: nn.Module, roi_heads: nn.Module, transform: nn.Module) -> None:
        super().__init__(backbone, rpn, roi_heads, transform)

    def forward(self, images, targets=None):
        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]
        """
        Args:
            images (list[Tensor]): images to be processed
            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)

        Returns:
            result (list[BoxList] or dict[Tensor]): the output from the model.
                During training, it returns a dict[Tensor] which contains the losses.
                During testing, it returns list[BoxList] contains additional fields
                like `scores`, `labels` and `mask` (for Mask R-CNN models).

        """
        original_image_sizes: List[Tuple[int, int]] = []
        for img in images:
            val = img.shape[-2:]
            torch._assert(
                len(val) == 2,
                f"expecting the last two dimensions of the Tensor to be H and W instead got {img.shape[-2:]}",
            )
            original_image_sizes.append((val[0], val[1]))

        images, targets = self.transform(images, targets)

        # Check for degenerate boxes
        features = self.backbone(images.tensors)
        # if isinstance(features, torch.Tensor):
        #     features = OrderedDict([("0", features)])

        proposals, proposal_losses = self.rpn(images, features, targets)
        # ic(proposals.shape)
        # ic(features.shape)
        features = self.roi_heads(features, proposals, images.image_sizes, targets)
        return features

class CustomFRCNN(ExtractingGeneralizedRCNN):
    def __init__(
        self,
        backbone,
        num_classes=None,
        # transform parameters
        min_size=800,
        max_size=1333,
        image_mean=None,
        image_std=None,
        # RPN parameters
        rpn_anchor_generator=None,
        rpn_head=None,
        rpn_pre_nms_top_n_train=2000,
        rpn_pre_nms_top_n_test=1000,
        rpn_post_nms_top_n_train=2000,
        rpn_post_nms_top_n_test=1000,
        rpn_nms_thresh=0.7,
        rpn_fg_iou_thresh=0.7,
        rpn_bg_iou_thresh=0.3,
        rpn_batch_size_per_image=256,
        rpn_positive_fraction=0.5,
        rpn_score_thresh=0.0,
        # Box parameters
        box_roi_pool=None,
        box_head=None,
        box_predictor=None,
        box_score_thresh=0.05,
        box_nms_thresh=0.5,
        box_detections_per_img=100,
        box_fg_iou_thresh=0.5,
        box_bg_iou_thresh=0.5,
        box_batch_size_per_image=512,
        box_positive_fraction=0.25,
        bbox_reg_weights=None,
        **kwargs,
    ):

        if not hasattr(backbone, "out_channels"):
            raise ValueError(
                "backbone should contain an attribute out_channels "
                "specifying the number of output channels (assumed to be the "
                "same for all the levels)"
            )

        if not isinstance(rpn_anchor_generator, (AnchorGenerator, type(None))):
            raise TypeError(
                f"rpn_anchor_generator should be of type AnchorGenerator or None instead of {type(rpn_anchor_generator)}"
            )
        if not isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None))):
            raise TypeError(
                f"box_roi_pool should be of type MultiScaleRoIAlign or None instead of {type(box_roi_pool)}"
            )

        if num_classes is not None:
            if box_predictor is not None:
                raise ValueError("num_classes should be None when box_predictor is specified")
        else:
            if box_predictor is None:
                raise ValueError("num_classes should not be None when box_predictor is not specified")

        out_channels = backbone.out_channels

        if rpn_anchor_generator is None:
            rpn_anchor_generator = _default_anchorgen()
        if rpn_head is None:
            rpn_head = RPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])

        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)
        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)

        rpn = RegionProposalNetwork(
            rpn_anchor_generator,
            rpn_head,
            rpn_fg_iou_thresh,
            rpn_bg_iou_thresh,
            rpn_batch_size_per_image,
            rpn_positive_fraction,
            rpn_pre_nms_top_n,
            rpn_post_nms_top_n,
            rpn_nms_thresh,
            score_thresh=rpn_score_thresh,
        )

        if box_roi_pool is None:
            box_roi_pool = MultiScaleRoIAlign(featmap_names=["0", "1", "2", "3"], output_size=7, sampling_ratio=2)

        if box_head is None:
            resolution = box_roi_pool.output_size[0]
            representation_size = 1024
            box_head = TwoMLPHead(out_channels * resolution**2, representation_size)

        if box_predictor is None:
            representation_size = 1024
            box_predictor = FastRCNNPredictor(representation_size, num_classes)

        roi_heads = ExtractingRoiHead(
            # Box
            box_roi_pool,
            box_head,
            box_predictor,
            box_fg_iou_thresh,
            box_bg_iou_thresh,
            box_batch_size_per_image,
            box_positive_fraction,
            bbox_reg_weights,
            box_score_thresh,
            box_nms_thresh,
            box_detections_per_img,
        )
        if image_mean is None:
            image_mean = [0.485, 0.456, 0.406]
        if image_std is None:
            image_std = [0.229, 0.224, 0.225]
        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std, **kwargs)

        super().__init__(backbone, rpn, roi_heads, transform)
        self.roi_heads = roi_heads
        # self.mod_roi_heads()

    def mod_roi_heads(self):
        self.roi_heads.box_predictor = Identity()
        self.roi_heads.box_head[6] = Identity()

weights_path = {
    "pretrained": "FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1",
    "weights_backbone": "ResNet50_Weights.IMAGENET1K_V1",
}
weights = FasterRCNN_ResNet50_FPN_V2_Weights.verify(weights_path['pretrained'])
weights_backbone = ResNet50_Weights.verify(weights_path['weights_backbone'])

backbone = resnet50(weights=weights_backbone, progress=True)
trainable_backbone_layers = _validate_trainable_layers(None, None, 5, 3)
backbone = _resnet_fpn_extractor(backbone, trainable_backbone_layers, norm_layer=nn.BatchNorm2d)
rpn_anchor_generator = _default_anchorgen()

anchor_sizes = ((32,), (64,), (128,), (256,), (512,))
aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)
anchor_generator = AnchorGenerator(
    sizes=anchor_sizes,
    aspect_ratios=aspect_ratios,
)

# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),
#                                    aspect_ratios=((0.5, 1.0, 2.0),))
rpn_head = RPNHead(backbone.out_channels, anchor_generator.num_anchors_per_location()[0], conv_depth=2)
box_head = FastRCNNConvFCHead(
    (backbone.out_channels, 7, 7), [256, 256, 256, 256], [1024], norm_layer=nn.BatchNorm2d
)
# roi_pooler = torchvision.ops.MultiScaleRoIAlign(
#     featmap_names=['0'],
#     output_size=7,
#     sampling_ratio=2
# )
# Load FasterRCNN Model
model = CustomFRCNN(
    backbone,
    num_classes=91,
    rpn_anchor_generator=anchor_generator,
    rpn_head=rpn_head,
    box_head=box_head,
    # box_roi_pool=roi_pooler,
)

if weights is not None:
    model.load_state_dict(weights.get_state_dict(progress=True, check_hash=True))


"""# **MODEL SETUP**"""

MIN_SIZE = 800
MAX_SIZE = 1333
def _image_transform(path):
    # img = Image.open(path)
    img = cv2.imread(path,1)
    im = np.array(img).astype(np.float32)
    # IndexError: too many indices for array, grayscale images
    if len(im.shape) < 3:
        im = np.repeat(im[:, :, np.newaxis], 3, axis=2)
    im = im[:, :, ::-1]
    im -= np.array([102.9801, 115.9465, 122.7717])
    im_shape = im.shape
    im_height = im_shape[0]
    im_width = im_shape[1]
    im_size_min = np.min(im_shape[0:2])
    im_size_max = np.max(im_shape[0:2])

    # Scale based on minimum size
    im_scale = MIN_SIZE / im_size_min

    # Prevent the biggest axis from being more than max_size
    # If bigger, scale it down
    if np.round(im_scale * im_size_max) > MAX_SIZE:
        im_scale = MAX_SIZE / im_size_max

    im = cv2.resize(
        im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR
    )
    img = torch.from_numpy(im).permute(2, 0, 1)

    im_info = {
        "width": im_width,
        "height": im_height
    }

    return img, im_scale, im_info

# model_torch = models.detection.fasterrcnn_resnet50_fpn(pretrained=ResNet50_Weights.IMAGENET1K_V1)
def save_npy(file_path, data):
    np.save(file_path, data)

if __name__=="__main__":
    device = "cuda:6"
    # List files
    path = "/data/npl/ViInfographicCaps/images/images"
    list_images = os.listdir(path)
    img_image_paths = [os.path.join(path, name) for name in list_images]
    save_dir = "/data/npl/ViInfographicCaps/features/fastercnn"
    # Testing an Image
    model = model.to(device)
    model.eval()
    # Load an Image
    for image_name, img_path in tqdm(zip(list_images, img_image_paths)):
        if not image_name.endswith(".png"):
            continue
        img, im_scale, im_info = _image_transform(img_path)
        image_id = image_name.split(".")[0]
        # img = model.transform([torch.tensor(img)])[0].tensors.squeeze(0)
        predictions = model(torch.tensor(img).unsqueeze(0).to(device))
        save_path = os.path.join(save_dir, f"{image_id}.npy")
        save_path_info = os.path.join(save_dir, f"{image_id}_info.npy")
        
        save_npy(save_path, predictions.detach().cpu().numpy())
        save_npy(save_path_info, im_info)
